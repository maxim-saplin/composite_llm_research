# Composite LLM: Idea & Design Document

## 1. Overview

**Composite LLM** is an extension architecture for `litellm` that enables "Logical Models"—abstractions that look like a single LLM to the client but actually orchestrate multiple underlying LLM calls behind the scenes.

The goal is to provide advanced capabilities like **Mixture-of-Agents (MoA)**, **System 2 Thinking**, and **Reinforcement Learning with Models (RLM)** while maintaining a unified, standard chat completion API.

### Core Value Proposition
- **Unified Interface**: Client code remains unchanged. Switching from `gpt-4o` to a complex `composite/moa/gpt-4o` pipeline requires only a config string change.
- **Model Agnostic**: Strategies can mix and match models from different providers (e.g., Anthropic for thinking, OpenAI for synthesis).
- **Introspection**: Built-in observability to track the "hidden" cost and latency of composite operations.

---

## 2. Architecture

The system is built around a **Custom Provider** pattern compatible with `litellm`.

```mermaid
graph LR
    Client[Client App] -->|litellm.completion| Proxy[Composite Provider]
    Proxy -->|Router| Strategy[Strategy Execution]
    
    subgraph Strategies
    Strategy -->|Select| MoA[Mixture of Agents]
    Strategy -->|Select| Think[Think Tool]
    Strategy -->|Select| RLM[RLM / Refinement]
    end
    
    MoA -->|Parallel Calls| Proposers["Proposer Models\n(GPT-3.5, Haiku, etc.)"]
    MoA -->|Synthesis| Aggregator["Aggregator Model\n(GPT-4o)"]
    
    Think -->|Thought Gen| Thinker["Model (Reasoning)"]
    Think -->|Final Ans| Answerer["Model (Answering)"]
```

### Components

1.  **CompositeLLMProvider**: The entry point. It intercepts calls where the model name starts with `composite/`. It parses the strategy and configuration from the model string.
    *   *Format*: `composite/<strategy>/<base_model>`
    *   *Example*: `composite/moa/gpt-4o`

2.  **BaseStrategy**: An abstract base class defining the contract for all orchestration logic.
    *   `execute(messages, model_config, optional_params, litellm_params)`

3.  **Observability Layer**: A callback system that logs granular metrics (latency, token usage, cost) for both the composite call and the sub-calls.

---

## 3. Supported Strategies

### A. Mixture-of-Agents (MoA)
Inspired by recent research (e.g., Together AI's MoA), this strategy leverages the collective intelligence of multiple models.

*   **Workflow**:
    1.  **Proposer Phase**: The user query is sent in parallel to a set of diverse, cheaper "proposer" models (e.g., Llama-3, Haiku, GPT-3.5).
    2.  **Aggregator Phase**: The responses from all proposers are collected and injected into the context of a capable "aggregator" model (e.g., GPT-4o or Claude 3.5 Sonnet).
    3.  **Synthesis**: The aggregator generates the final response, correcting errors and combining insights.
*   **Config**: `proposers` (list of models), `aggregator` (base model).

### B. Think Strategy (System 2)
Emulates the "thinking" process of reasoning models.

*   **Workflow**:
    1.  **Thought Generation**: The model is prompted to output an internal monologue inside `<thinking>` tags, analyzing the problem before answering.
    2.  **Extraction**: The thought block is parsed (and optionally hidden or stored).
    3.  **Final Response**: The model (or a different one) is prompted to provide the final answer based on the generated thoughts.
*   **Benefits**: Reduces hallucination on complex logic tasks.

### C. RLM (Reinforcement Learning with Models) / Refinement
*Planned for future iteration.*

*   **Workflow**:
    1.  **Generate**: Initial draft.
    2.  **Critique/Reward**: A separate "Reward Model" or prompt evaluates the draft against criteria.
    3.  **Refine**: If the score is low, the model is prompted to improve the draft based on the critique. This loop repeats until a threshold is met.

---

## 4. Technical Implementation Details

### Integration with LiteLLM
We utilize `litellm`'s custom provider capabilities.

```python
# Pseudo-code usage
resp = litellm.completion(
    model="composite/moa/gpt-4o",
    messages=[{"role": "user", "content": "Complex query..."}],
    # Custom params passed through
    proposers=["claude-3-haiku", "gpt-3.5-turbo"]
)
```

### Tool Call Handling
Handling tool calls in a composite architecture is non-trivial.

*   **Strategy 1 (Pass-through)**: If the underlying model (e.g., the Aggregator in MoA) generates a tool call, it is bubbled up to the client. The client executes the tool and sends the result back. The Composite Provider must be state-aware to route the tool output back to the correct step in the strategy.
*   **Strategy 2 (Internal Execution)**: The Composite Provider handles the tool execution internally (agentic loop) and only returns the final text result. *This is more complex but hides implementation details.*
*   **Current Design**: Focuses on Text-to-Text. Tool calls generated by the *final* model in the chain will be returned to the client.

---

## 5. Observability & Dashboard

Since one API call now triggers $N$ calls, cost and latency transparency is critical.

*   **Logging**: All sub-calls are logged to a structured `jsonl` file.
*   **Dashboard (Streamlit)**:
    *   **Aggregated Metrics**: Total cost per composite call.
    *   **Trace View**: Timeline showing the parallel execution of MoA proposers or the sequential steps of RLM.
    *   **Latency Analysis**: Identify which sub-model is the bottleneck.

## 6. Directory Structure

```text
composite_llm_research/
├── composite_llm/
│   ├── provider.py       # Main Provider Class
│   ├── strategies/       # Strategy Implementations
│   │   ├── base.py
│   │   ├── moa.py
│   │   └── think.py
│   └── observability.py  # Logging Callbacks
├── dashboard.py          # Streamlit Visualization
├── demo.py               # Usage Examples
└── DESIGN.md             # This file
```

---

## 7. Quick Start & Testing

This section explains how to set up the environment using `uv`, run the demo, and view the observability dashboard.

### 7.1 Prerequisites & Setup

Ensure you have `uv` installed. If not, install it via `curl -LsSf https://astral.sh/uv/install.sh | sh`.

1.  **Create a virtual environment**:
    ```bash
    uv venv
    source .venv/bin/activate
    ```

2.  **Install dependencies**:
    ```bash
    uv pip install -e .
    ```

### 7.2 Environment Configuration

Create a `.env` file in the root directory (you can copy `sample.env`):

```bash
cp sample.env .env
```

Edit `.env` to add your API keys. You can use standard provider keys (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`) or `LITELLM_API_KEY` for general use.

Example `.env`:
```text
LITELLM_API_KEY=csk-...
MODEL=cerebras/zai-glm-4.6
```

*Note: If no API key is provided, `demo.py` will run in **Mock Mode**, simulating responses without making actual network calls.*

### 7.3 Running the Demo

The `demo.py` script showcases two strategies: **Think** (System 2 reasoning) and **MoA** (Mixture of Agents).

```bash
python demo.py
```

Expected output:
1.  **Think Strategy**: It will query the model (or mock) asking for thoughts before the final answer.
2.  **MoA Strategy**: It will simulate querying multiple proposer models and then aggregating their responses.
3.  **Logging**: All interactions are automatically logged to `llm_logs.jsonl` in the current directory.

### 7.4 Running the Observability Dashboard

To visualize the metrics (latency, token usage, model distribution), start the Streamlit dashboard:

```bash
streamlit run dashboard.py
```

This will open a local web server (usually at `http://localhost:8501`) where you can see:
-   Total calls and error rates.
-   Latency distributions.
-   A table of recent logs (which updates as you run more demos).

### 7.5 Manual Testing (Python Shell)

You can also test interactively by importing the wrapper from `demo.py`:

```python
import os
import sys
sys.path.append(os.getcwd())
from demo import composite_completion

# Ensure API key is set
# os.environ["OPENAI_API_KEY"] = "..."

# Test a simple composite call
resp = composite_completion(
    model="composite/think/gpt-4o",
    messages=[{"role": "user", "content": "Why is the sky blue?"}],
    optional_params={"include_thoughts": True}
)
print(resp.choices[0].message.content)
```
